{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819f3415",
   "metadata": {},
   "source": [
    "pip install pandas numpy scikit-learn spacy transformers torch matplotlib seaborn jupyter\n",
    "python -m spacy download es_core_news_sm  # Modelo de español para spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50d03d",
   "metadata": {},
   "source": [
    "## Preprocesamiento del texto\n",
    "\n",
    "Convierte archivos `.xlsx` en texto limpio para entrenar a los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60505f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Biblioteca para manipulación de datos\n",
    "import re  # Biblioteca para expresiones regulares\n",
    "#from sklearn.model_selection import train_test_split # Esta función se utiliza para dividir el conjunto de datos en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "# Cargar el conjunto de datos de entrenamiento, prueba y dev \n",
    "train_df = pd.read_excel('data/train.xlsx')\n",
    "test_df = pd.read_excel('data/test.xlsx')\n",
    "dev_df = pd.read_excel('data/development.xlsx')\n",
    "\n",
    "# Función de limpieza\n",
    "def limpia_texo(texto):\n",
    "    texto = str(texto).lower()  # Convertir a minúsculas\n",
    "    texto = re.sub(r'http\\S+|ww\\S+|@\\w+|#\\w+', '', texto)  # Eliminar URLs, menciones y hashtags\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar caracteres especiales\n",
    "    return texto\n",
    "\n",
    "# Crear una nueva columna 'clean_text' en el DataFrame 'train_df' aplicando la función 'limpia_texo' a la columna 'Text'\n",
    "train_df['texto_limpio'] = train_df['Text'].apply(limpia_texo)\n",
    "test_df['texto_limpio'] = test_df['TEXT'].apply(limpia_texo)\n",
    "dev_df['texto_limpio'] = dev_df['Text'].apply(limpia_texo)\n",
    "\n",
    "# Se corrige la diferencia de valores en la columna 'Category' del DataFrame 'test_df'\n",
    "test_map = {True: 1, False: 0}\n",
    "test_df['CATEGORY'] = test_df['CATEGORY'].map(test_map)\n",
    "train_map = {\"True\": 1, \"Fake\": 0}\n",
    "train_df['Category'] = train_df['Category'].map(train_map)\n",
    "dev_df['Category'] = dev_df['Category'].map(train_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0645580",
   "metadata": {},
   "source": [
    "## Implementación de modelo Naive Bayes + TF-IDF\n",
    "\n",
    "Se implementa el modelo Naive Bayes utilizanddo TF--IDF para la clasificación del texto. Este será el modelo clásico de referencia para comparar con los modelos de BERT y RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5b26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.60       286\n",
      "           1       0.61      0.74      0.67       286\n",
      "\n",
      "    accuracy                           0.64       572\n",
      "   macro avg       0.64      0.64      0.63       572\n",
      "weighted avg       0.64      0.64      0.63       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Convierte texto a vectores\n",
    "from sklearn.naive_bayes import MultinomialNB # Clasificador Naive Bayes\n",
    "from sklearn.metrics import classification_report # Genera un informe de clasificación del modelo\n",
    "\n",
    "# Se vectoriza el texto limpio\n",
    "vectorizer = TfidfVectorizer(max_features=5000) # Se limita a 5000 características\n",
    "x_train = vectorizer.fit_transform(train_df['texto_limpio'])\n",
    "x_test = vectorizer.transform(test_df['texto_limpio'])\n",
    "\n",
    "# Se entrena al modelo\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(x_train, train_df['Category'])\n",
    "\n",
    "# Se evalúa el modelo\n",
    "y_pred = model_nb.predict(x_test)\n",
    "reporte_nb = classification_report(test_df['CATEGORY'], y_pred) \n",
    "print(reporte_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4f0f5",
   "metadata": {},
   "source": [
    "## Implementación avanzado BERT\n",
    "\n",
    "Se implementa el modelo BERT para la clasificación del texto. Este modelo es más avanzado, fine-tuning de un transformer, y se espera que tenga un mejor rendimiento en comparación con el modelo Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014acdb",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abeead8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\PC\\OneDrive\\Escritorio\\Universidad\\Semestre 10\\AyPIT\\DetectorFakeNews_AyPIT_2025-2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_25004\\1989014552.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 10:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692104</td>\n",
       "      <td>0.518644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.599404</td>\n",
       "      <td>0.691525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708296</td>\n",
       "      <td>0.674576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\OneDrive\\Escritorio\\Universidad\\Semestre 10\\AyPIT\\DetectorFakeNews_AyPIT_2025-2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_25004\\1989014552.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
      "c:\\Users\\PC\\OneDrive\\Escritorio\\Universidad\\Semestre 10\\AyPIT\\DetectorFakeNews_AyPIT_2025-2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_25004\\1989014552.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
      "c:\\Users\\PC\\OneDrive\\Escritorio\\Universidad\\Semestre 10\\AyPIT\\DetectorFakeNews_AyPIT_2025-2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_25004\\1989014552.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=255, training_loss=0.6090862199371936, metrics={'train_runtime': 634.1143, 'train_samples_per_second': 3.198, 'train_steps_per_second': 0.402, 'total_flos': 133397305067520.0, 'train_loss': 0.6090862199371936, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch # Biblioteca para trabajar con modelos de lenguaje preentrenados\n",
    "from sklearn.metrics import accuracy_score # Métrica de precisión\n",
    "\n",
    "# Carga el tokenizador y el modelo BERT\n",
    "# Use a valid Hugging Face model identifier\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Crea un mapeo de etiquetas a valores numéricos\n",
    "label_map = {'FAKE': 0, 'REAL': 1} \n",
    "\n",
    "# Función para tokenizar el texto\n",
    "def tokenizar_texto(texto):\n",
    "    return tokenizer(texto['texto_limpio'].tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_tokens = tokenizar_texto(train_df)\n",
    "dev_tokens = tokenizar_texto(dev_df)\n",
    "\n",
    "# Se crea un Dataset personalizado para BERT\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "# Se crea el NewsDataset para entrenamiento y prueba\n",
    "train_dataset = NewsDataset(train_tokens, train_df['Category'].tolist())\n",
    "dev_dataset = NewsDataset(dev_tokens, dev_df['Category'].tolist())\n",
    "\n",
    "# Se configuran los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directorio de salida\n",
    "    num_train_epochs=3,  # Número de épocas de entrenamiento\n",
    "    per_device_train_batch_size=8,  # Tamaño del batch\n",
    "    eval_strategy=\"epoch\",  # Estrategia de evaluación\n",
    ")\n",
    "\n",
    "# Función para calcular la precisión\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids # Se obtienen las etiquetas verdaderas\n",
    "    preds = pred.predictions.argmax(-1) # argmax devuelve el índice del valor máximo a lo largo de un eje\n",
    "    acc = accuracy_score(labels, preds) # Se calcula la precisión de acuerdo a las etiquetas verdaderas y las predicciones\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# Se crea el entrenador\n",
    "trainer = Trainer(\n",
    "    model = model, # Modelo a entrenar\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset, # Conjunto de datos de entrenamiento\n",
    "    eval_dataset = dev_dataset, # Conjunto de datos de evaluación\n",
    "    compute_metrics = compute_metrics, # Función para calcular la precisión\n",
    ")\n",
    "\n",
    "trainer.train() # Se entrena el modelo\n",
    "\n",
    "# Se guarda el modelo\n",
    "# model.save_pretrained('model/modelo_preentrenado')\n",
    "# tokenizer.save_pretrained('model/tokenizador_preentrenado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e281cc",
   "metadata": {},
   "source": [
    "Se ejecuta el entrenamiento del modelo BERT utilizando el conjunto de datos preprocesado. Se utiliza la biblioteca `transformers` de Hugging Face para cargar el modelo y realizar el fine-tuning. *******************AJUSTAR**********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d08e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\OneDrive\\Escritorio\\Universidad\\Semestre 10\\AyPIT\\DetectorFakeNews_AyPIT_2025-2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_25004\\1989014552.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0626661777496338, 'eval_accuracy': 0.534965034965035, 'eval_runtime': 31.906, 'eval_samples_per_second': 17.928, 'eval_steps_per_second': 2.257, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Se evalúa el modelo\n",
    "test_tokens = tokenizar_texto(test_df)\n",
    "test_dataset = NewsDataset(test_tokens, test_df['CATEGORY'].tolist()) # Se crea el conjunto de datos de prueba\n",
    "resultados = trainer.evaluate(test_dataset) # Se evalúa el modelo\n",
    "\n",
    "print(resultados) # Se imprimen los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c689335",
   "metadata": {},
   "source": [
    "## Comparación de resultados\n",
    "Se comparan los resultados de los modelos Naive Bayes y BERT utilizando métricas como la precisión, la recuperación y la puntuación F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da21c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.534965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Modelo Precision\n",
       "0  Naive Bayes          \n",
       "1         BERT  0.534965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = {\n",
    "    \"Modelo\": [\"Naive Bayes\", \"BERT\"],\n",
    "    \"Precision\": [reporte_nb[0], resultados['eval_accuracy']],\n",
    "}\n",
    "\n",
    "resultados_df = pd.DataFrame(data) # Se crea un DataFrame con los resultados\n",
    "\n",
    "display(resultados_df) # Se muestran los resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
